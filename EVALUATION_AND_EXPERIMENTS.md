# RAG ë„ì„œ ì¶”ì²œ ì‹œìŠ¤í…œ - í‰ê°€ ë° ì‹¤í—˜ ê°€ì´ë“œ

## ğŸ“Œ ê°œìš”

ì´ ë¬¸ì„œëŠ” RAG ë„ì„œ ì¶”ì²œ ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ì„ ì²´ê³„ì ìœ¼ë¡œ í‰ê°€í•˜ê³  ë‹¤ì–‘í•œ íŒŒë¼ë¯¸í„° ì¡°í•©ì„ ì‹¤í—˜í•  ìˆ˜ ìˆëŠ” í†µí•© ê°€ì´ë“œì…ë‹ˆë‹¤.

### ì£¼ìš” ê¸°ëŠ¥

- **ì²´ê³„ì  í‰ê°€**: 30ê°œì˜ ë‹¤ì–‘í•œ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ë¡œ ì¥ë¥´ ì í•©ë„, ê²€ìƒ‰ í’ˆì§ˆ ì¸¡ì •
- **ë°°ì¹˜ ì‹¤í—˜**: ì—¬ëŸ¬ ì„¤ì •ì„ í•œ ë²ˆì— ì‹¤í—˜í•˜ê³  ìë™ìœ¼ë¡œ ë¹„êµ
- **ì‹œê°í™”**: ì‹¤í—˜ ê²°ê³¼ë¥¼ ê·¸ë˜í”„ë¡œ ë¹„êµí•˜ì—¬ ìµœì  ì„¤ì • ë„ì¶œ
- **Orchestrator í†µí•©**: ëª¨í˜¸í•œ ì¿¼ë¦¬ ì²˜ë¦¬ ëŠ¥ë ¥ í‰ê°€

---

## ğŸ—ï¸ ì‹œìŠ¤í…œ êµ¬ì¡°

### í•µì‹¬ ì»´í¬ë„ŒíŠ¸

```
í‰ê°€ ì‹œìŠ¤í…œ
â”œâ”€â”€ evaluation_dataset.py       # 30ê°œì˜ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ë°ì´í„°ì…‹
â”œâ”€â”€ evaluation_metrics.py       # ì¥ë¥´ ì í•©ë„, ê²€ìƒ‰ í’ˆì§ˆ ë©”íŠ¸ë¦­
â”œâ”€â”€ run_evaluation.py          # ë‹¨ì¼ í‰ê°€ ì‹¤í–‰
â””â”€â”€ evaluation_viz.py          # ê²°ê³¼ ì‹œê°í™”

ì‹¤í—˜ ì‹œìŠ¤í…œ
â”œâ”€â”€ experiment_config.py       # ì‹¤í—˜ ì„¤ì • ì •ì˜ (Presets í¬í•¨)
â”œâ”€â”€ run_experiments.py         # ë°°ì¹˜ ì‹¤í—˜ ì‹¤í–‰
â””â”€â”€ compare_experiments.py     # ì‹¤í—˜ ê²°ê³¼ ë¹„êµ ë° ì‹œê°í™”

Orchestrator (ëª¨í˜¸ì„± ì²˜ë¦¬)
â”œâ”€â”€ orchestrator.py           # ìƒíƒœ ë¨¸ì‹  ê¸°ë°˜ Orchestrator
â””â”€â”€ chains.py                 # ì „ë¬¸í™”ëœ LLM Chainë“¤
```

### íŒŒì¼ ê°„ ê´€ê³„

```
run_evaluation.py
    â†“ uses
evaluation_dataset.py + evaluation_metrics.py
    â†“ produces
evaluation_results/*.json

run_experiments.py
    â†“ uses
experiment_config.py + run_evaluation.py
    â†“ produces
experiment_results/comparison_*.json

compare_experiments.py
    â†“ uses
experiment_results/comparison_*.json
    â†“ produces
comparison_viz/*.png
```

---

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### 1. í™˜ê²½ ì¤€ë¹„

```bash
# í™˜ê²½ í™œì„±í™”
eval "$(mamba shell hook --shell bash)" && mamba activate bkms

# Vector store í™•ì¸ (í•„ìš”ì‹œ ìƒì„±)
python main.py --stats-only
```

### 2. ì²« í‰ê°€ ì‹¤í–‰ (3ë¶„ ì†Œìš”)

```bash
# ìƒ˜í”Œ 5ê°œë¡œ ë¹ ë¥´ê²Œ í…ŒìŠ¤íŠ¸
python run_evaluation.py --sample 5 --k 5
```

### 3. ì²« ì‹¤í—˜ ì‹¤í–‰ (5ë¶„ ì†Œìš”)

```bash
# Baseline ì‹¤í—˜
python run_experiments.py --preset baseline --sample 5
```

### 4. ê²°ê³¼ í™•ì¸

```bash
# ê²°ê³¼ íŒŒì¼ í™•ì¸
ls -lt evaluation_results/
ls -lt experiment_results/

# ì‹œê°í™” ìƒì„±
python compare_experiments.py \
  --comparison-file experiment_results/comparison_*.json
```

---

## ğŸ“Š í‰ê°€ ì‹œìŠ¤í…œ (Evaluation)

### í‰ê°€ ë°ì´í„°ì…‹

**30ê°œì˜ ë‹¤ì–‘í•œ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬** í¬í•¨:

- **SPECIFIC (5ê°œ)**: "SF ì†Œì„¤ ì¶”ì²œí•´ì¤˜", "ë§ˆì¼€íŒ… ê´€ë ¨ ì‹¤ìš©ì„œ í•„ìš”í•´"
- **EMOTIONAL (5ê°œ)**: "ìš”ì¦˜ ë„ˆë¬´ ìš°ìš¸í•´", "ë¬´ê¸°ë ¥í•œë° ë™ê¸°ë¶€ì—¬ ë°›ê³  ì‹¶ì–´"
- **SITUATIONAL (5ê°œ)**: "êµ°ëŒ€ ê°€ê¸° ì „ì— ì½ì„ ì±…", "ì¶œí‡´ê·¼í•  ë•Œ ì½ì„ë§Œí•œ ê±°"
- **VAGUE (5ê°œ)**: "ì¸ìƒì— ë„ì›€ë˜ëŠ” ì±…", "ì¬ë°ŒëŠ” ì±… ì¶”ì²œ"
- **MULTI_INTENT (5ê°œ)**: "ì¬ë°Œê³  ì˜ë¯¸ ìˆëŠ” ì†Œì„¤", "ê°€ë³ê²Œ ì½íˆë©´ì„œ ìƒê°í•  ê±°ë¦¬ë¥¼ ì£¼ëŠ” ì±…"
- **ê¸°íƒ€ (5ê°œ)**: ë‹¤ì–‘í•œ ì¶”ê°€ ì¼€ì´ìŠ¤

ê° ì¿¼ë¦¬ëŠ” ë‹¤ìŒ ì •ë³´ë¥¼ í¬í•¨:
- ê¸°ëŒ€ ì¥ë¥´ (ground truth)
- ê¸°ëŒ€ í…Œë§ˆ/ë¶„ìœ„ê¸° (ì„ íƒì )
- ê´€ë ¨ ë„ì„œ ë¦¬ìŠ¤íŠ¸ (ì„ íƒì )

### í‰ê°€ ë©”íŠ¸ë¦­

#### ì¥ë¥´ ì í•©ë„ ë©”íŠ¸ë¦­ (GenreEvaluator)

```python
- Genre Precision: ê²€ìƒ‰ëœ ì±… ì¤‘ ì˜¬ë°”ë¥¸ ì¥ë¥´ ë¹„ìœ¨ (0-1)
- Genre Recall: ê¸°ëŒ€ ì¥ë¥´ë¥¼ ì–¼ë§ˆë‚˜ ì»¤ë²„í–ˆëŠ”ê°€ (0-1)
- Genre F1 Score: Precisionê³¼ Recallì˜ ì¡°í™” í‰ê· 
- Genre Diversity: ì¥ë¥´ ë‹¤ì–‘ì„± (unique genres / total books)
```

**í•´ì„ ê°€ì´ë“œ:**
- **Precision >= 0.8**: ë§¤ìš° ì¢‹ìŒ - ê²€ìƒ‰ ê²°ê³¼ê°€ ì •í™•
- **Recall >= 0.8**: ë§¤ìš° ì¢‹ìŒ - ê¸°ëŒ€ ì¥ë¥´ë¥¼ ì˜ ì»¤ë²„
- **F1 Score >= 0.7**: ì „ë°˜ì ìœ¼ë¡œ ìš°ìˆ˜í•œ ì„±ëŠ¥
- **Diversity >= 0.7**: ë§¤ìš° ë‹¤ì–‘í•¨

#### ê²€ìƒ‰ í’ˆì§ˆ ë©”íŠ¸ë¦­ (RetrievalEvaluator)

```python
- Precision@K: ìƒìœ„ Kê°œ ì¤‘ ê´€ë ¨ ë¬¸ì„œ ë¹„ìœ¨
- Recall@K: ì „ì²´ ê´€ë ¨ ë¬¸ì„œ ì¤‘ ìƒìœ„ Kê°œì— í¬í•¨ëœ ë¹„ìœ¨
- MRR (Mean Reciprocal Rank): ì²« ë²ˆì§¸ ê´€ë ¨ ë¬¸ì„œ ìˆœìœ„ì˜ ì—­ìˆ˜
- Coverage: ê¸°ëŒ€ ë„ì„œë¥¼ ì–¼ë§ˆë‚˜ ê²€ìƒ‰í–ˆëŠ”ê°€
```

#### ì˜ë¯¸ ìœ ì‚¬ë„ ë©”íŠ¸ë¦­ (SemanticEvaluator)

```python
- Average Similarity: ì¿¼ë¦¬ì™€ ê²€ìƒ‰ ê²°ê³¼ ê°„ í‰ê·  ì½”ì‚¬ì¸ ìœ ì‚¬ë„
- Max Similarity: ìµœê³  ìœ ì‚¬ë„ (ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ê²°ê³¼)
- Min Similarity: ìµœì € ìœ ì‚¬ë„
```

### í‰ê°€ ì‹¤í–‰ ë°©ë²•

#### ê¸°ë³¸ ì‚¬ìš©ë²•

```bash
# ì „ì²´ ë°ì´í„°ì…‹ í‰ê°€ (k=5)
python run_evaluation.py --k 5

# íŠ¹ì • ì¿¼ë¦¬ íƒ€ì…ë§Œ í‰ê°€
python run_evaluation.py --query-type emotional --k 3

# ìƒ˜í”Œ í‰ê°€ (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš©)
python run_evaluation.py --sample 10 --k 5

# ê²°ê³¼ ì €ì¥ ì•ˆ í•¨ (ì½˜ì†” ì¶œë ¥ë§Œ)
python run_evaluation.py --no-save --sample 5
```

#### ì£¼ìš” ì˜µì…˜

- `--k`: ê²€ìƒ‰í•  ë¬¸ì„œ ê°œìˆ˜ (ê¸°ë³¸ê°’: 5)
- `--query-type`: í‰ê°€í•  ì¿¼ë¦¬ íƒ€ì… (all, specific, emotional, situational, vague, multi_intent)
- `--sample`: ìƒ˜í”Œë§í•  ì¿¼ë¦¬ ê°œìˆ˜
- `--output-dir`: ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: evaluation_results)
- `--no-save`: ê²°ê³¼ íŒŒì¼ ì €ì¥ ì•ˆ í•¨

### í‰ê°€ ê²°ê³¼ íŒŒì¼

í‰ê°€ ì‹¤í–‰ í›„ `evaluation_results/` ë””ë ‰í† ë¦¬ì— ìƒì„±:

```
evaluation_results/
â”œâ”€â”€ detailed_results_{timestamp}.json     # ê° ì¿¼ë¦¬ë³„ ìƒì„¸ ê²°ê³¼
â”œâ”€â”€ aggregated_results_{timestamp}.json   # ì „ì²´ í‰ê·  ë©”íŠ¸ë¦­
â””â”€â”€ summary_report_{timestamp}.txt        # ì‚¬ëŒì´ ì½ê¸° ì‰¬ìš´ ìš”ì•½ ë¦¬í¬íŠ¸
```

### ê²°ê³¼ ì‹œê°í™”

```bash
# ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„± (ëª¨ë“  ì‹œê°í™” í¬í•¨)
python evaluation_viz.py \
  --results-file evaluation_results/detailed_results_TIMESTAMP.json \
  --output-dir evaluation_results

# ì‹¤íŒ¨ ë¶„ì„ (F1 < 0.5ì¸ ì¿¼ë¦¬ ë¶„ì„)
python evaluation_viz.py \
  --results-file evaluation_results/detailed_results_TIMESTAMP.json \
  --failure-threshold 0.5
```

**ìƒì„±ë˜ëŠ” ì‹œê°í™”:**
1. `metrics_by_query_type.png`: ì¿¼ë¦¬ íƒ€ì…ë³„ ë©”íŠ¸ë¦­ ë¹„êµ
2. `f1_distribution.png`: F1 ìŠ¤ì½”ì–´ ë¶„í¬ ë° ë°•ìŠ¤í”Œë¡¯
3. `genre_distribution.png`: ê²€ìƒ‰ëœ ì¥ë¥´ ë¶„í¬
4. `correlation_matrix.png`: ë©”íŠ¸ë¦­ ê°„ ìƒê´€ê´€ê³„

---

## ğŸ§ª ì‹¤í—˜ ì‹œìŠ¤í…œ (Experiments)

### ì‹¤í—˜ ì„¤ì • (ExperimentConfig)

ì‹¤í—˜ì€ ë‹¤ìŒ ìš”ì†Œë“¤ì˜ ì¡°í•©ìœ¼ë¡œ ì •ì˜ë©ë‹ˆë‹¤:

#### RetrievalConfig (ê²€ìƒ‰ íŒŒë¼ë¯¸í„°)

```python
k: int = 5                    # ê²€ìƒ‰ ë¬¸ì„œ ìˆ˜
use_mmr: bool = True          # MMR ë‹¤ì–‘ì„± ê²€ìƒ‰
mmr_lambda: float = 0.8       # 0=max diversity, 1=max relevance
use_reranking: bool = True    # ë² ìŠ¤íŠ¸ì…€ëŸ¬ ê¸°ë°˜ ì¬ì •ë ¬
rank_alpha: float = 0.8       # ìœ ì‚¬ë„ ê°€ì¤‘ì¹˜
rank_beta: float = 0.2        # ë² ìŠ¤íŠ¸ì…€ëŸ¬ ìˆœìœ„ ê°€ì¤‘ì¹˜
use_adaptive_k: bool = True   # ì ì‘í˜• K
min_k: int = 2                # ìµœì†Œ K
max_k: int = 10               # ìµœëŒ€ K
```

#### OrchestratorConfig (Orchestrator ì„¤ì •)

```python
enabled: bool = False         # Orchestrator ì‚¬ìš© ì—¬ë¶€
model_name: str = None        # ìµœì¢… ì¶”ì²œìš© LLM ëª¨ë¸
chain_model_name: str = None  # Chain ì—°ì‚°ìš© ê²½ëŸ‰ ëª¨ë¸
verbose: bool = False         # ìƒì„¸ ë¡œê·¸
```

### ì‹¤í—˜ Preset

ì‹¤í—˜ ì‹œìŠ¤í…œì€ ìì£¼ ì‚¬ìš©í•˜ëŠ” ì„¤ì • ì¡°í•©ì„ Presetìœ¼ë¡œ ì œê³µí•©ë‹ˆë‹¤.

#### 1. `baseline` - ê¸°ë³¸ ì„±ëŠ¥ ì¸¡ì •

```bash
python run_experiments.py --preset baseline --sample 10
```

- MMR: ON (Î»=0.8)
- Reranking: ON (Î±=0.8, Î²=0.2)
- Adaptive K: ON
- Orchestrator: OFF

**ì‚¬ìš© ëª©ì **: í˜„ì¬ ì‹œìŠ¤í…œì˜ ê¸°ë³¸ ì„±ëŠ¥ íŒŒì•…

#### 2. `orchestrator` - Orchestrator íš¨ê³¼ ë¹„êµ

```bash
python run_experiments.py --preset orchestrator --sample 10
```

- Baseline vs Orchestrator enabled 2ê°€ì§€ ì‹¤í—˜
- ëª¨í˜¸í•œ ì¿¼ë¦¬ ì²˜ë¦¬ ëŠ¥ë ¥ í–¥ìƒ í™•ì¸

**ì‚¬ìš© ëª©ì **: Orchestratorì˜ ì„±ëŠ¥ ê°œì„  íš¨ê³¼ ì¸¡ì •

#### 3. `ablation` - Ablation Study (ê¸°ëŠ¥ë³„ ì˜í–¥)

```bash
python run_experiments.py --preset ablation --sample 10
```

6ê°œ ì‹¤í—˜:
1. Minimal (ëª¨ë“  ê¸°ëŠ¥ OFF)
2. Minimal + MMR
3. Minimal + Reranking
4. Minimal + Adaptive K
5. All features (Baseline)
6. All features + Orchestrator

**ì‚¬ìš© ëª©ì **: ê° ê¸°ëŠ¥ì´ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ ë¶„ì„

#### 4. `k_sweep` - K ê°’ ë³€í™”

```bash
python run_experiments.py --preset k_sweep --sample 10
```

5ê°œ ì‹¤í—˜: k = 2, 3, 5, 7, 10

**ì‚¬ìš© ëª©ì **: ê²€ìƒ‰ ë¬¸ì„œ ê°œìˆ˜ê°€ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥

#### 5. `lambda_sweep` - Diversity vs Relevance

```bash
python run_experiments.py --preset lambda_sweep --sample 10
```

6ê°œ ì‹¤í—˜: Î» = 0.3, 0.5, 0.7, 0.8, 0.9, 0.95

- **Î» ë‚®ìŒ (0.3-0.5)**: ë‹¤ì–‘ì„± â†‘, ì •í™•ë„ â†“
- **Î» ë†’ìŒ (0.8-0.95)**: ê´€ë ¨ì„± â†‘, ë‹¤ì–‘ì„± â†“

**ì‚¬ìš© ëª©ì **: ë‹¤ì–‘ì„±ê³¼ ê´€ë ¨ì„±ì˜ íŠ¸ë ˆì´ë“œì˜¤í”„ ë¶„ì„

#### 6. `rerank_sweep` - Reranking ê°€ì¤‘ì¹˜ ì¡°í•©

```bash
python run_experiments.py --preset rerank_sweep --sample 10
```

5ê°œ ì‹¤í—˜: (Î±, Î²) = (0.5, 0.5), (0.6, 0.4), (0.7, 0.3), (0.8, 0.2), (0.9, 0.1)

- **Î± ë†’ìŒ**: ì˜ë¯¸ ìœ ì‚¬ë„ ì¤‘ì‹¬
- **Î² ë†’ìŒ**: ë² ìŠ¤íŠ¸ì…€ëŸ¬ ìˆœìœ„ ì¤‘ì‹¬

**ì‚¬ìš© ëª©ì **: ìœ ì‚¬ë„ì™€ ì¸ê¸°ë„ì˜ ìµœì  ë°¸ëŸ°ìŠ¤ ì°¾ê¸°

### ì‹¤í—˜ ì‹¤í–‰ ë°©ë²•

#### ê¸°ë³¸ ì‚¬ìš©ë²•

```bash
# Baseline ì‹¤í—˜
python run_experiments.py --preset baseline --sample 10

# ì—¬ëŸ¬ Preset ë™ì‹œ ì‹¤í–‰
python run_experiments.py --preset ablation --sample 10
```

#### ê³ ê¸‰ ì˜µì…˜

```bash
# íŠ¹ì • ì¿¼ë¦¬ íƒ€ì…ë§Œ í‰ê°€
python run_experiments.py \
  --preset baseline \
  --query-types emotional situational \
  --sample 10

# ì¶œë ¥ ë””ë ‰í† ë¦¬ ë³€ê²½
python run_experiments.py \
  --preset k_sweep \
  --output-dir my_experiments \
  --sample 10

# ê°œë³„ ê²°ê³¼ ì €ì¥ ìƒëµ (ë¹„êµ ë°ì´í„°ë§Œ)
python run_experiments.py \
  --preset k_sweep \
  --no-save-individual
```

### ì‹¤í—˜ ê²°ê³¼ êµ¬ì¡°

```
experiment_results/
â”œâ”€â”€ comparison_20251216_123456.json          # ë¹„êµ ë°ì´í„°
â”œâ”€â”€ baseline__orchestrator_off_k_5_mmr_Î»0.8/ # ê° ì‹¤í—˜ ë””ë ‰í† ë¦¬
â”‚   â”œâ”€â”€ config.json                          # ì‹¤í—˜ ì„¤ì •
â”‚   â”œâ”€â”€ detailed_results_*.json              # ìƒì„¸ ê²°ê³¼
â”‚   â”œâ”€â”€ aggregated_results_*.json            # ì§‘ê³„ ê²°ê³¼
â”‚   â””â”€â”€ summary_report_*.txt                 # ìš”ì•½ ë¦¬í¬íŠ¸
â””â”€â”€ comparison_viz/                          # ë¹„êµ ì‹œê°í™”
    â”œâ”€â”€ overall_comparison.png               # ì „ì²´ ë©”íŠ¸ë¦­ ë¹„êµ
    â”œâ”€â”€ f1_ranking.png                       # F1 ìˆœìœ„
    â”œâ”€â”€ feature_impact.png                   # ê¸°ëŠ¥ë³„ ì˜í–¥
    â””â”€â”€ tradeoff_analysis.png                # íŠ¸ë ˆì´ë“œì˜¤í”„ ë¶„ì„
```

---

## ğŸ“ˆ ì‹¤í—˜ ê²°ê³¼ ë¹„êµ ë° ì‹œê°í™”

### ë¹„êµ ë„êµ¬ ì‚¬ìš©

```bash
# ì‹¤í—˜ ê²°ê³¼ ë¹„êµ ë° ì‹œê°í™”
python compare_experiments.py \
  --comparison-file experiment_results/comparison_TIMESTAMP.json

# ì»¤ìŠ¤í…€ ì¶œë ¥ ë””ë ‰í† ë¦¬
python compare_experiments.py \
  --comparison-file experiment_results/comparison_TIMESTAMP.json \
  --output-dir my_comparison_results
```

### ìƒì„±ë˜ëŠ” ì‹œê°í™”

#### 1. Overall Comparison (overall_comparison.png)

ëª¨ë“  ì‹¤í—˜ì˜ Precision, Recall, F1, Diversityë¥¼ ë‚˜ë€íˆ ë¹„êµí•˜ëŠ” ë§‰ëŒ€ ê·¸ë˜í”„

#### 2. F1 Ranking (f1_ranking.png)

F1 ìŠ¤ì½”ì–´ ê¸°ì¤€ìœ¼ë¡œ ì‹¤í—˜ ìˆœìœ„ë¥¼ í‘œì‹œí•˜ëŠ” ìˆ˜í‰ ë§‰ëŒ€ ê·¸ë˜í”„

#### 3. Feature Impact (feature_impact.png)

ê° ê¸°ëŠ¥(MMR, Reranking, Adaptive K, Orchestrator)ì˜ ì˜í–¥ë„ë¥¼ ë¹„êµ

#### 4. Tradeoff Analysis (tradeoff_analysis.png)

- Precision-Recall íŠ¸ë ˆì´ë“œì˜¤í”„ ì‚°ì ë„
- Diversity-F1 íŠ¸ë ˆì´ë“œì˜¤í”„ ì‚°ì ë„

### ì½˜ì†” ì¶œë ¥ ì˜ˆì‹œ

```
====================================================================================================
EXPERIMENT COMPARISON
====================================================================================================

Experiment                                Precision     Recall         F1  Diversity
----------------------------------------------------------------------------------------------------
orchestrator__on__adaptive_k_2-10_...         0.750      0.700      0.723      0.650
baseline__orchestrator_off_k_5_mmr_...        0.720      0.680      0.698      0.640

====================================================================================================
```

---

## ğŸ”¬ Orchestrator í†µí•©

### Orchestratorë€?

**Ambiguity-Aware Orchestrator**ëŠ” ëª¨í˜¸í•œ ë„ì„œ ì¶”ì²œ ìš”ì²­ì„ ì²˜ë¦¬í•˜ëŠ” ìƒíƒœ ë¨¸ì‹  ê¸°ë°˜ ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤.

### ì£¼ìš” íŠ¹ì§•

1. **ëª¨í˜¸ì„± ìë™ ê°ì§€**: "ìš”ì¦˜ ë„ˆë¬´ ê³µí—ˆí•´" ê°™ì€ ê°ì • ê¸°ë°˜ ì¿¼ë¦¬ ì¸ì‹
2. **Query Rewriting**: ëª¨í˜¸í•œ ì¿¼ë¦¬ë¥¼ ê²€ìƒ‰ ìµœì í™”ëœ ë¬¸ì¥ìœ¼ë¡œ ë³€í™˜
3. **í’ˆì§ˆ í‰ê°€**: ê²€ìƒ‰ ê²°ê³¼ê°€ ì¶©ë¶„í•œì§€ ìì²´ í‰ê°€
4. **ëª…í™•í™” ì§ˆë¬¸**: í•„ìš”ì‹œ ì‚¬ìš©ìì—ê²Œ ì¶”ê°€ ì •ë³´ ìš”ì²­ (ìµœì†Œí™”)
5. **ìµœì¢… ì¶”ì²œ**: ë§¥ë½ì„ ê³ ë ¤í•œ ì¹œê·¼í•œ ì¶”ì²œ ìƒì„±

### ì²˜ë¦¬ íë¦„

```
User Query
    â†“
[1] Ambiguity Detection
    â†“
[2] Query Rewriting (if ambiguous)
    â†“
[3] Retrieve from Vector DB
    â†“
[4] Quality Evaluation
    â†“
[5] Clarification (if insufficient) â†’ User Response â†’ [3]
    â†“
[6] Final Recommendation
```

### Orchestrator ì‹¤í—˜

```bash
# Orchestrator ìœ ë¬´ ë¹„êµ
python run_experiments.py --preset orchestrator --sample 10

# ëª¨í˜¸í•œ ì¿¼ë¦¬ì—ì„œë§Œ í…ŒìŠ¤íŠ¸
python run_experiments.py \
  --preset orchestrator \
  --query-types emotional vague situational \
  --sample 10
```

**ì˜ˆìƒ ê²°ê³¼:**
- Orchestratorê°€ ëª¨í˜¸í•œ ì¿¼ë¦¬(emotional, vague)ì—ì„œ F1 í–¥ìƒ
- Query rewritingìœ¼ë¡œ ë” ê´€ë ¨ì„± ë†’ì€ ë¬¸ì„œ ê²€ìƒ‰
- ì²˜ë¦¬ ì‹œê°„ ì¦ê°€ (ì¶”ê°€ LLM í˜¸ì¶œ)

### ì„¸ë¶€ êµ¬í˜„

#### Chains (chains.py)

5ê°œì˜ ì „ë¬¸í™”ëœ Chain:
1. **AmbiguityDetector**: ëª¨í˜¸ì„± ê°ì§€ ë° ë¶„ë¥˜
2. **QueryRewriter**: ê²€ìƒ‰ ìµœì í™” ì¿¼ë¦¬ ìƒì„±
3. **RetrieveQualityEvaluator**: ê²€ìƒ‰ ê²°ê³¼ í’ˆì§ˆ í‰ê°€
4. **ClarificationQuestionGenerator**: ëª…í™•í™” ì§ˆë¬¸ ìƒì„±
5. **FinalRecommender**: ìµœì¢… ì¶”ì²œ ìƒì„±

#### ëª¨ë¸ ì‚¬ìš© ì „ëµ

ì„±ëŠ¥ê³¼ ë¹„ìš© ìµœì í™”ë¥¼ ìœ„í•´ ë‘ ê°€ì§€ ëª¨ë¸ ì‚¬ìš©:

- **Chain ì—°ì‚°ìš© (ê²½ëŸ‰)**: `gemini-2.0-flash-lite` - ë¹ ë¥¸ íŒë‹¨/ë¶„ë¥˜
- **ìµœì¢… ì¶”ì²œìš© (ê³ í’ˆì§ˆ)**: `gemini-2.0-flash` - ë†’ì€ í’ˆì§ˆì˜ ì¶”ì²œ í…ìŠ¤íŠ¸

---

## ğŸ’¡ ì¶”ì²œ ì›Œí¬í”Œë¡œìš°

### ì‹œë‚˜ë¦¬ì˜¤ 1: ì²« í‰ê°€ (ìƒˆ ì‹œìŠ¤í…œ or ë³€ê²½ í›„)

```bash
# 1. ë¹ ë¥¸ ìƒ˜í”Œ í…ŒìŠ¤íŠ¸
python run_evaluation.py --sample 5 --k 5

# 2. ì „ì²´ í‰ê°€
python run_evaluation.py --k 5

# 3. ì‹œê°í™”
python evaluation_viz.py \
  --results-file evaluation_results/detailed_results_*.json
```

### ì‹œë‚˜ë¦¬ì˜¤ 2: Orchestrator íš¨ê³¼ ê²€ì¦

```bash
# 1. Baseline vs Orchestrator ë¹„êµ
python run_experiments.py --preset orchestrator --sample 10

# 2. ëª¨í˜¸í•œ ì¿¼ë¦¬ì—ì„œ ì§‘ì¤‘ í…ŒìŠ¤íŠ¸
python run_experiments.py \
  --preset orchestrator \
  --query-types emotional vague \
  --sample 10

# 3. ê²°ê³¼ ë¹„êµ
python compare_experiments.py \
  --comparison-file experiment_results/comparison_*.json
```

### ì‹œë‚˜ë¦¬ì˜¤ 3: íŒŒë¼ë¯¸í„° íŠœë‹

```bash
# 1. Ablation studyë¡œ ì¤‘ìš” ê¸°ëŠ¥ íŒŒì•…
python run_experiments.py --preset ablation --sample 10

# 2. ì¤‘ìš” íŒŒë¼ë¯¸í„° sweep
python run_experiments.py --preset k_sweep --sample 10
python run_experiments.py --preset lambda_sweep --sample 10

# 3. ìµœì  ì¡°í•© ì°¾ê¸°
python compare_experiments.py \
  --comparison-file experiment_results/comparison_*.json
```

### ì‹œë‚˜ë¦¬ì˜¤ 4: ì¿¼ë¦¬ íƒ€ì…ë³„ ìµœì í™”

```bash
# ê° ì¿¼ë¦¬ íƒ€ì…ë³„ë¡œ ì‹¤í—˜
for qtype in emotional situational vague specific multi_intent; do
    python run_experiments.py \
      --preset k_sweep \
      --query-types $qtype \
      --sample 10 \
      --output-dir experiments_${qtype}
done

# ì¿¼ë¦¬ íƒ€ì…ë³„ ìµœì  ì„¤ì • ë¶„ì„
```

---

## ğŸ” ê²°ê³¼ í•´ì„ ê°€ì´ë“œ

### Baseline vs Orchestrator ë¹„êµ

**í™•ì¸ ì‚¬í•­:**
- Emotional, vague ì¿¼ë¦¬ì—ì„œ F1 ê°œì„ ë˜ì—ˆëŠ”ê°€?
- Overall F1ì´ ì¼ê´€ë˜ê²Œ í–¥ìƒë˜ì—ˆëŠ”ê°€?
- ì²˜ë¦¬ ì‹œê°„ ì¦ê°€ëŠ” í—ˆìš© ê°€ëŠ¥í•œê°€?

### K ê°’ ì‹¤í—˜

- **K ì‘ìŒ (2-3)**: Precision â†‘, Recall â†“, Diversity â†“
- **K í¼ (7-10)**: Precision â†“, Recall â†‘, Diversity â†‘
- **ìµœì ê°’**: ëŒ€ë¶€ë¶„ k=5ê°€ ì ì ˆ, Adaptive Kë¡œ ìë™ ì¡°ì • ê¶Œì¥

### MMR Lambda ì‹¤í—˜

- **Î» = 0.3-0.5 (High Diversity)**: ë‹¤ì–‘ì„± ì¤‘ì‹œ, ì¶”ì²œ ë‹¤ì–‘í™”
- **Î» = 0.8-0.95 (High Relevance)**: ì •í™•ë„ ì¤‘ì‹œ, ê´€ë ¨ì„± ë†’ì€ ê²°ê³¼
- **ìµœì ê°’**: Î»=0.7~0.8ì´ ì¢‹ì€ ê· í˜•

### Reranking ê°€ì¤‘ì¹˜

- **Î± ë†’ìŒ (0.8-0.9)**: ì˜ë¯¸ ìœ ì‚¬ë„ ì¤‘ì‹¬
- **Î² ë†’ìŒ (0.3-0.5)**: ë² ìŠ¤íŠ¸ì…€ëŸ¬ ì¸ê¸°ë„ ì¤‘ì‹¬
- **ì¶”ì²œ**: Î±=0.7-0.8 (ì˜ë¯¸ ìœ ì‚¬ë„ ìš°ì„ )

---

## ğŸ› ï¸ ê³ ê¸‰ ì‚¬ìš©ë²•

### ì»¤ìŠ¤í…€ ì‹¤í—˜ ì •ì˜

Python ì½”ë“œë¡œ ì§ì ‘ ì‹¤í—˜ ì •ì˜:

```python
from experiment_config import ExperimentConfig, RetrievalConfig, OrchestratorConfig
from run_experiments import ExperimentBatchRunner

# ì»¤ìŠ¤í…€ ì‹¤í—˜ ì •ì˜
custom_exp = ExperimentConfig(
    name="my_experiment",
    description="High diversity with orchestrator",
    retrieval=RetrievalConfig(
        k=7,
        use_mmr=True,
        mmr_lambda=0.5,  # High diversity
        use_reranking=True,
        rank_alpha=0.9,
        use_adaptive_k=False,
    ),
    orchestrator=OrchestratorConfig(
        enabled=True,
        verbose=False,
    ),
    sample_size=20,
)

# ì‹¤í—˜ ì‹¤í–‰
runner = ExperimentBatchRunner()
result = runner.run_single_experiment(custom_exp)
```

### í‰ê°€ ë°ì´í„°ì…‹ í™•ì¥

ìƒˆë¡œìš´ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ì¶”ê°€ ([evaluation_dataset.py](evaluation_dataset.py)):

```python
TestQuery(
    query_id="NEW001",
    query="ë‹¹ì‹ ì˜ ìƒˆë¡œìš´ ì¿¼ë¦¬",
    query_type=QueryType.SPECIFIC,
    expected_genres=[GenreCategory.NOVEL],
    expected_themes=["í…Œë§ˆ1", "í…Œë§ˆ2"],
    relevant_books=["ì±… ì œëª©1", "ì±… ì œëª©2"],  # ì„ íƒì 
    notes="ì¶”ê°€ ì„¤ëª…"
)
```

### ìƒˆë¡œìš´ ë©”íŠ¸ë¦­ ì¶”ê°€

[evaluation_metrics.py](evaluation_metrics.py)ì— ìƒˆ í‰ê°€ì í´ë˜ìŠ¤ ì¶”ê°€:

```python
class CustomEvaluator:
    """Custom evaluation metric."""

    def evaluate(self, test_query, retrieved_books):
        # Your custom evaluation logic
        pass
```

---

## ğŸ”§ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### "Vector store not found" ì—ëŸ¬

```bash
# Vector store ë¨¼ì € ìƒì„±
python main.py --stats-only
```

### ì‹¤í—˜ì´ ë„ˆë¬´ ëŠë¦¼

```bash
# ìƒ˜í”Œ ìˆ˜ ì¤„ì´ê¸°
python run_experiments.py --preset baseline --sample 3

# ë˜ëŠ” orchestrator ì œì™¸
python run_experiments.py --preset k_sweep --sample 5
```

### ë©”ëª¨ë¦¬ ë¶€ì¡±

```bash
# í•œ ë²ˆì— í•˜ë‚˜ì”© ì‹¤í—˜ ì‹¤í–‰
python run_experiments.py --preset baseline --sample 5
```

### Orchestrator ê´€ë ¨ ì—ëŸ¬

- `AmbiguityDetector` ê²°ê³¼ í™•ì¸: verbose=Trueë¡œ ì‹¤í–‰
- `confidence` threshold ì¡°ì •
- Query rewriting í’ˆì§ˆ í™•ì¸

---

## ğŸ“‹ ì²´í¬ë¦¬ìŠ¤íŠ¸

### ì‹¤í—˜ ì‹œì‘ ì „

- [ ] Vector store ìƒì„± ì™„ë£Œ
- [ ] í™˜ê²½ í™œì„±í™” (mamba activate bkms)
- [ ] ì‹¤í—˜ ëª©ì  ëª…í™•íˆ ì •ì˜
- [ ] ë¨¼ì € ì‘ì€ ìƒ˜í”Œë¡œ í…ŒìŠ¤íŠ¸ (--sample 3~5)

### ì‹¤í—˜ ì‹¤í–‰ ì¤‘

- [ ] ë¡œê·¸ ë©”ì‹œì§€ í™•ì¸
- [ ] ì˜¤ë¥˜ ì—†ì´ ì™„ë£Œë˜ëŠ”ì§€ ì²´í¬
- [ ] í•œ ë²ˆì— í•˜ë‚˜ì˜ ë³€ìˆ˜ë§Œ ë³€ê²½ (Ablation study í™œìš©)

### ì‹¤í—˜ ì™„ë£Œ í›„

- [ ] ê²°ê³¼ íŒŒì¼ ìƒì„± í™•ì¸
- [ ] ì‹œê°í™” ìƒì„± ë° í™•ì¸
- [ ] F1 ranking ë¶„ì„
- [ ] Tradeoff ê·¸ë˜í”„ë¡œ ê· í˜•ì  ì°¾ê¸°
- [ ] ì¿¼ë¦¬ íƒ€ì…ë³„ ì„±ëŠ¥ ì°¨ì´ í™•ì¸
- [ ] ìµœì  ì„¤ì • ì„ íƒ ë° ë¬¸ì„œí™”

---

## ğŸ“ ê°œì„  ë°©í–¥ ì œì•ˆ

í‰ê°€ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒê³¼ ê°™ì€ ê°œì„ ì„ ê³ ë ¤:

### ë‚®ì€ Recall

- ê²€ìƒ‰ k ê°’ ì¦ê°€
- Query rewriting ê°œì„ 
- ì„ë² ë”© ëª¨ë¸ ë³€ê²½

### ë‚®ì€ Precision

- Reranking ì•Œê³ ë¦¬ì¦˜ ê°œì„ 
- ë©”íƒ€ë°ì´í„° í•„í„°ë§ ê°•í™”
- MMR íŒŒë¼ë¯¸í„° ì¡°ì •

### ë‚®ì€ Diversity

- MMR lambda ê°’ ê°ì†Œ (ë” ë§ì€ ë‹¤ì–‘ì„±)
- ì¥ë¥´ ë°¸ëŸ°ì‹± ì „ëµ ë„ì…

### íŠ¹ì • ì¿¼ë¦¬ íƒ€ì… ì„±ëŠ¥ ì €í•˜

- í•´ë‹¹ íƒ€ì…ì— íŠ¹í™”ëœ Query rewriting
- ì¿¼ë¦¬ íƒ€ì…ë³„ ê²€ìƒ‰ ì „ëµ ë¶„í™”
- Orchestrator í”„ë¡¬í”„íŠ¸ ê°œì„ 

---

## ğŸ“š ê´€ë ¨ íŒŒì¼

### í‰ê°€ ì‹œìŠ¤í…œ

- [evaluation_dataset.py](evaluation_dataset.py) - í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ë°ì´í„°ì…‹
- [evaluation_metrics.py](evaluation_metrics.py) - í‰ê°€ ë©”íŠ¸ë¦­
- [run_evaluation.py](run_evaluation.py) - í‰ê°€ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
- [evaluation_viz.py](evaluation_viz.py) - ì‹œê°í™” ë„êµ¬

### ì‹¤í—˜ ì‹œìŠ¤í…œ

- [experiment_config.py](experiment_config.py) - ì‹¤í—˜ ì„¤ì • ë° Presets
- [run_experiments.py](run_experiments.py) - ë°°ì¹˜ ì‹¤í—˜ ì‹¤í–‰
- [compare_experiments.py](compare_experiments.py) - ê²°ê³¼ ë¹„êµ ë° ì‹œê°í™”

### Orchestrator

- [orchestrator.py](orchestrator.py) - Orchestrator êµ¬í˜„
- [chains.py](chains.py) - ì „ë¬¸í™”ëœ Chainë“¤

### ì„¤ì •

- [config.py](config.py) - ì‹œìŠ¤í…œ ì„¤ì •

---

## âœ¨ ìš”ì•½

ì´ í†µí•© ë¬¸ì„œëŠ” RAG ë„ì„œ ì¶”ì²œ ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ì„ ì²´ê³„ì ìœ¼ë¡œ í‰ê°€í•˜ê³  ìµœì ì˜ ì„¤ì •ì„ ì°¾ê¸° ìœ„í•œ ì™„ì „í•œ ê°€ì´ë“œì…ë‹ˆë‹¤.

**í•µì‹¬ í¬ì¸íŠ¸:**
1. 30ê°œì˜ ë‹¤ì–‘í•œ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ë¡œ ì‹¤ì œ ì‚¬ìš© íŒ¨í„´ ë°˜ì˜
2. ì¥ë¥´ ì í•©ë„, ê²€ìƒ‰ í’ˆì§ˆ, ì˜ë¯¸ ìœ ì‚¬ë„ë¥¼ ë‹¤ê°ë„ë¡œ í‰ê°€
3. Presetì„ í™œìš©í•œ ë¹ ë¥¸ ì‹¤í—˜ ë° ë¹„êµ
4. Orchestratorë¡œ ëª¨í˜¸í•œ ì¿¼ë¦¬ ì²˜ë¦¬ ëŠ¥ë ¥ í–¥ìƒ
5. ì‹œê°í™”ë¥¼ í†µí•œ ì§ê´€ì ì¸ ê²°ê³¼ ë¶„ì„

**ì‹œì‘í•˜ê¸°:**
```bash
# 1. ë¹ ë¥¸ í…ŒìŠ¤íŠ¸
python run_experiments.py --preset baseline --sample 5

# 2. ê²°ê³¼ ë¹„êµ
python compare_experiments.py --comparison-file experiment_results/comparison_*.json

# 3. ìµœì  ì„¤ì • ì„ íƒ ë° ì ìš©
```

Happy Experimenting! ğŸ§ªâœ¨
