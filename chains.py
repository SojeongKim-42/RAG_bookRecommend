"""
Chain modules for ambiguity-aware book recommendation.
Each chain is a specialized LLM call for a specific decision-making step.
"""

from typing import Dict, Any, List, Optional
from urllib.parse import quote_plus
from langchain.chat_models import init_chat_model
from langchain_core.messages import SystemMessage, HumanMessage
from config import Config


def generate_google_shopping_link(book_title: str, author: str = None) -> str:
    """
    Generate Google Shopping search link for a book.

    Args:
        book_title: Book title
        author: Author name (optional)

    Returns:
        Google Shopping search URL
    """
    # Create search query
    search_query = book_title
    if author:
        search_query = f"{book_title} {author}"

    # Add "ì±… êµ¬ë§¤" to search query for better results
    search_query = f"{search_query} ì±… êµ¬ë§¤"

    # URL encode
    encoded_query = quote_plus(search_query)

    # Google Shopping search URL
    return f"https://www.google.com/search?q={encoded_query}&tbm=shop"


def generate_search_links(book_title: str, author: str = None) -> Dict[str, str]:
    """
    Generate multiple shopping links for a book.

    Args:
        book_title: Book title
        author: Author name (optional)

    Returns:
        Dictionary with platform names and URLs
    """
    search_query = book_title
    if author:
        search_query = f"{book_title} {author}"

    encoded_query = quote_plus(search_query)

    return {
        "google_shopping": f"https://www.google.com/search?q={encoded_query}+ì±…+êµ¬ë§¤&tbm=shop",
        "google_search": f"https://www.google.com/search?q={encoded_query}+ì±…",
        "yes24": f"https://www.yes24.com/Product/Search?query={encoded_query}",
        "aladin": f"https://www.aladin.co.kr/search/wsearchresult.aspx?SearchTarget=All&SearchWord={encoded_query}",
    }


class AmbiguityDetector:
    """Detects whether a query is ambiguous and classifies the type."""

    AMBIGUITY_TYPES = [
        "emotional_only",        # ex: "ìš”ì¦˜ ë„ˆë¬´ ê³µí—ˆí•´"
        "situational",           # ex: "êµ°ëŒ€ ê°€ê¸° ì „ì— ì½ì„ ì±…"
        "vague_topic",           # ex: "ì¸ìƒì— ë„ì›€ë˜ëŠ” ì±…"
        "multi_intent",          # ex: "ì¬ë°Œê³  ì˜ë¯¸ ìˆëŠ” ì†Œì„¤"
        "not_ambiguous"          # ëª…í™•í•œ ìš”ì²­
    ]

    def __init__(self, model_name: str = None):
        self.model_name = model_name or Config.CHAIN_MODEL_NAME
        self.model = None

    def _get_model(self):
        if self.model is None:
            self.model = init_chat_model(self.model_name)
        return self.model

    def detect(self, user_query: str, chat_history: List[Dict[str, str]] = None) -> Dict[str, Any]:
        """
        Detect ambiguity in user query, considering conversation context.

        Returns:
            {
                "is_ambiguous": bool,
                "ambiguity_type": str,
                "confidence": float (0-1),
                "reason": str
            }
        """
        history_text = ""
        if chat_history:
            # Format last few turns for context
            relevant_history = chat_history[-6:]  # Last 3 turns
            history_text = "\n".join([f"{msg['role']}: {msg['content']}" for msg in relevant_history])

        system_prompt = f"""ë‹¹ì‹ ì€ ë„ì„œ ì¶”ì²œ ìš”ì²­ì˜ ëª¨í˜¸ì„±ì„ íŒë³„í•˜ëŠ” ë¶„ë¥˜ê¸°ì…ë‹ˆë‹¤.

                        **ëŒ€í™” ë§¥ë½**:
                        {history_text if history_text else "ì—†ìŒ"}

                        **íŒë‹¨ ê¸°ì¤€**:
                        1. ì‚¬ìš©ìê°€ **ì´ì „ ì§ˆë¬¸ì— ëŒ€í•´ êµ¬ì²´ì ì¸ ë‹µë³€**ì„ í–ˆë‹¤ë©´ `not_ambiguous`ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”.
                           (ì˜ˆ: AI "ì–´ë–¤ ì¥ë¥´ê°€ ì¢‹ìœ¼ì„¸ìš”?" -> ì‚¬ìš©ì "ì†Œì„¤ì´ìš”" => **ëª…í™•í•¨**)
                        2. ë¬¸ë§¥ì„ ê³ ë ¤í–ˆì„ ë•Œ ì—¬ì „íˆ ì •ë³´ê°€ ë¶€ì¡±í•˜ë©´ í•´ë‹¹ë˜ëŠ” ëª¨í˜¸ì„± íƒ€ì…ì„ ì„ íƒí•˜ì„¸ìš”.

                        ì‚¬ìš©ì ì§ˆë¬¸ì„ ë‹¤ìŒ ì¹´í…Œê³ ë¦¬ ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”:

                        1. emotional_only: ê°ì •ë§Œ í‘œí˜„í•˜ê³  êµ¬ì²´ì ì¸ ì„ í˜¸ë„ê°€ ì—†ìŒ
                        ì˜ˆ: "ìš”ì¦˜ ë„ˆë¬´ ê³µí—ˆí•´", "ê¸°ë¶„ì´ ì•ˆ ì¢‹ì•„"

                        2. situational: ìƒí™©ë§Œ ì„¤ëª…í•˜ê³  ì¥ë¥´/ìŠ¤íƒ€ì¼ ì–¸ê¸‰ ì—†ìŒ
                        ì˜ˆ: "êµ°ëŒ€ ê°€ê¸° ì „ì— ì½ì„ ì±…", "ì¶œí‡´ê·¼í•  ë•Œ ì½ì„ë§Œí•œ ê±°"

                        3. vague_topic: ì£¼ì œê°€ ë„ˆë¬´ ê´‘ë²”ìœ„í•˜ê±°ë‚˜ ì¶”ìƒì 
                        ì˜ˆ: "ì¸ìƒì— ë„ì›€ë˜ëŠ” ì±…", "ì„±ì¥í•  ìˆ˜ ìˆëŠ” ì±…"

                        4. multi_intent: ì—¬ëŸ¬ ìš”êµ¬ì‚¬í•­ì´ ì„ì—¬ìˆê³  ìš°ì„ ìˆœìœ„ ë¶ˆëª…í™•
                        ì˜ˆ: "ì¬ë°Œê³  ì˜ë¯¸ ìˆê³  ì§§ì€ ì†Œì„¤", "ê°ë™ì ì´ë©´ì„œ ìœ ìµí•œ ì±…"

                        5. not_ambiguous: ì¥ë¥´, ì£¼ì œ, ìŠ¤íƒ€ì¼ì´ ëª…í™•í•¨ (ë˜ëŠ” ë¬¸ë§¥ìƒ ëª…í™•í•´ì§)
                        ì˜ˆ: "SF ì†Œì„¤ ì¶”ì²œí•´ì¤˜", "ì†Œì„¤" (ì´ì „ ì§ˆë¬¸ì´ ì¥ë¥´ì˜€ì„ ë•Œ)

                        JSON í˜•ì‹ìœ¼ë¡œ ë‹µí•˜ì„¸ìš”:
                        {{
                        "is_ambiguous": true/false,
                        "ambiguity_type": "ì¹´í…Œê³ ë¦¬",
                        "confidence": 0.0~1.0,
                        "reason": "íŒë‹¨ ì´ìœ "
                        }}"""

        model = self._get_model()

        messages = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=f"ì‚¬ìš©ì ì§ˆë¬¸: {user_query}")
        ]

        response = model.invoke(messages)

        # Parse response (assuming structured output)
        import json
        try:
            # Extract JSON from response
            content = response.content
            # Find JSON in markdown code blocks if present
            if "```json" in content:
                content = content.split("```json")[1].split("```")[0].strip()
            elif "```" in content:
                content = content.split("```")[1].split("```")[0].strip()

            result = json.loads(content)
            return result
        except (json.JSONDecodeError, IndexError) as e:
            # Fallback: assume ambiguous
            return {
                "is_ambiguous": True,
                "ambiguity_type": "vague_topic",
                "confidence": 0.5,
                "reason": f"íŒŒì‹± ì‹¤íŒ¨: {str(e)}"
            }


class QueryRewriter:
    """Rewrites ambiguous queries into search-optimized versions."""

    def __init__(self, model_name: str = None):
        self.model_name = model_name or Config.CHAIN_MODEL_NAME
        self.model = None

    def _get_model(self):
        if self.model is None:
            self.model = init_chat_model(self.model_name)
        return self.model

    def rewrite(self, user_query: str, chat_history: List[Dict[str, str]], ambiguity_type: str) -> str:
        """
        Rewrite query for better vector search using conversation history.

        Args:
            user_query: Original user query
            chat_history: Conversation history
            ambiguity_type: Type of ambiguity detected

        Returns:
            Rewritten query optimized for vector search
        """
        history_text = ""
        if chat_history:
            relevant_history = chat_history[-6:]
            history_text = "\n".join([f"{msg['role']}: {msg['content']}" for msg in relevant_history])

        system_prompt = f"""ë‹¹ì‹ ì€ ê²€ìƒ‰ ì¿¼ë¦¬ ìµœì í™” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

                        ì‚¬ìš©ìì˜ ìš”ì²­ì„ **ë²¡í„° ê²€ìƒ‰ì— ìµœì í™”ëœ ë¬¸ì¥**ìœ¼ë¡œ ë³€í™˜í•˜ì„¸ìš”.
                        ì´ì „ ëŒ€í™” ë§¥ë½ì„ ê³ ë ¤í•˜ì—¬, **ëˆ„ì ëœ ì‚¬ìš©ìì˜ ìš”êµ¬ì‚¬í•­ì„ ëª¨ë‘ í¬í•¨**í•´ì•¼ í•©ë‹ˆë‹¤.

                        **ëŒ€í™” ë§¥ë½**:
                        {history_text if history_text else "ì—†ìŒ"}

                        **í˜„ì¬ ëª¨í˜¸ì„± ìœ í˜•**: {ambiguity_type}

                        **ì‘ì—… ì§€ì¹¨**:
                        1. ì´ì „ ëŒ€í™”ì—ì„œ ì‚¬ìš©ìê°€ ì–¸ê¸‰í•œ ì„ í˜¸ë„(ì¥ë¥´, ë¶„ìœ„ê¸°, ë¶„ëŸ‰ ë“±)ë¥¼ ëª¨ë‘ ê¸°ì–µí•˜ì„¸ìš”.
                        2. í˜„ì¬ ìš”ì²­ê³¼ ì´ì „ ìš”êµ¬ì‚¬í•­ì„ í•©ì³ì„œ **í•˜ë‚˜ì˜ êµ¬ì²´ì ì¸ ê²€ìƒ‰ ë¬¸ì¥**ì„ ë§Œë“œì„¸ìš”.
                           (ì˜ˆ: ì´ì „="ì†Œì„¤ ì¶”ì²œí•´ì¤˜", í˜„ì¬="ë°ì€ ë¶„ìœ„ê¸°" -> "ë°ê³  í¬ë§ì°¬ ë¶„ìœ„ê¸°ì˜ ì¥í¸ ì†Œì„¤, í•´í”¼ì—”ë”©")
                        3. ë¶ˆí•„ìš”í•œ ì¸ì‚¬ë§ì´ë‚˜ ì„œìˆ ì–´ëŠ” ì œì™¸í•˜ê³  í‚¤ì›Œë“œ ìœ„ì£¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.
                        4. **ì¤‘ìš”**: ì‚¬ìš©ìê°€ ì¥ë¥´(ì†Œì„¤, ì—ì„¸ì´ ë“±)ë¥¼ ëª…ì‹œí•˜ì§€ ì•Šì•˜ë‹¤ë©´, ì„ì˜ë¡œ ì¥ë¥´ë¥¼ ë‹¨ì • ì§“ì§€ ë§ê³  **ë¶„ìœ„ê¸°ì™€ ì£¼ì œ ìœ„ì£¼**ë¡œ ì‘ì„±í•˜ì„¸ìš”. "ì±…"ì´ë‚˜ "ë„ì„œ" ê°™ì€ í¬ê´„ì  í‘œí˜„ì„ ì‚¬ìš©í•˜ì„¸ìš”.

                        **ë³€í™˜ ì „ëµ**:
                        1. emotional_only â†’ ê°ì •ì— ë§ëŠ” ë¶„ìœ„ê¸°/í‚¤ì›Œë“œ ì¶”ê°€ (ì¥ë¥´ ê³ ì • X)
                        2. situational â†’ ìƒí™©ì— ë§ëŠ” ë…ì„œ ìŠ¤íƒ€ì¼/ë¶„ëŸ‰ ì¶”ê°€
                        3. vague_topic â†’ êµ¬ì²´ì ì¸ í•˜ìœ„ ì£¼ì œ/ê´€ì  ì¶”ê°€
                        4. multi_intent â†’ ìš°ì„ ìˆœìœ„ ëª…ì‹œ + êµ¬ì²´ì  ì¡°ê±´

                        **ì¶œë ¥ í˜•ì‹**:
                        - í˜•ìš©ì‚¬ + ìƒí™© + ì¥ë¥´ + ì œì•½ì¡°ê±´
                        - í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ

                        **ì˜¤ì§ rewritten queryë§Œ ì¶œë ¥í•˜ì„¸ìš”. ì„¤ëª…ì´ë‚˜ ì¶”ì²œì€ ê¸ˆì§€.**"""

        model = self._get_model()

        messages = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=f"ì‚¬ìš©ì ìš”ì²­: {user_query}")
        ]

        response = model.invoke(messages)
        return response.content.strip()


class RetrieveQualityEvaluator:
    """Evaluates whether retrieved documents are sufficient."""

    def __init__(self, model_name: str = None):
        self.model_name = model_name or Config.CHAIN_MODEL_NAME
        self.model = None

    def _get_model(self):
        if self.model is None:
            self.model = init_chat_model(self.model_name)
        return self.model

    def evaluate(self, user_query: str, retrieved_books: List[Dict[str, Any]], ambiguity_type: str = "not_ambiguous") -> Dict[str, Any]:
        """
        Evaluate quality of retrieved documents.

        Args:
            user_query: User query
            retrieved_books: List of retrieved books
            ambiguity_type: Type of ambiguity detected

        Returns:
            {
                "sufficient": bool,
                "reason": str,
                "missing_info": List[str]
            }
        """
        # Format books for evaluation
        books_summary = "\n".join([
            f"{i+1}. [{book.get('êµ¬ë¶„', 'N/A')}] {book.get('ìƒí’ˆëª…', 'N/A')}"
            for i, book in enumerate(retrieved_books)
        ])

        # Conditional Logic based on Ambiguity Type
        if ambiguity_type == "not_ambiguous":
            # Permissive logic (Enhanced F1 optimized)
            criteria = """
                        1. **ê´€ë ¨ì„±**: ê²€ìƒ‰ëœ ì±…ë“¤ ì¤‘ ì‚¬ìš©ìì˜ ì˜ë„ì™€ ì¡°ê¸ˆì´ë¼ë„ ì—°ê´€ëœ ì±…ì´ ìˆëŠ”ê°€?
                        2. **ìµœì†Œ ì¶©ì¡±**: ì™„ë²½í•˜ì§€ ì•Šë”ë¼ë„, ì‚¬ìš©ìì—ê²Œ ì¶”ì²œí•´ ì¤„ ë§Œí•œ í›„ë³´ê°€ 1ê¶Œì´ë¼ë„ ìˆëŠ”ê°€?
                        3. **ë‹¤ì–‘ì„± ì¸ì •**: ì¥ë¥´ë‚˜ í…Œë§ˆê°€ ë‹¤ì–‘í•˜ë”ë¼ë„, ê¸ì •ì ìœ¼ë¡œ í‰ê°€í•˜ë¼.
                        """
            strictness_instruction = "ì •ë§ë¡œ ì—‰ëš±í•œ ì±…ë§Œ ìˆê±°ë‚˜, ì¶”ì²œí•  ë§Œí•œ ì±…ì´ ë‹¨ í•œ ê¶Œë„ ì—†ì„ ë•Œë§Œ `sufficient: false`ë¡œ íŒë‹¨í•˜ì„¸ìš”."
        else:
            # Strict logic for vague/situational/multi_intent/emotional_only (Triggers Clarification)
            criteria = """
                        1. **êµ¬ì²´ì  ì í•©ì„±**: ê²€ìƒ‰ ê²°ê³¼ê°€ ì‚¬ìš©ìì˜ ë³µì¡í•˜ê±°ë‚˜ ëª¨í˜¸í•œ ìš”êµ¬(ìƒí™©, ë‹¤ì¤‘ ì˜ë„)ë¥¼ **ëª…í™•íˆ í•´ì†Œ**í•´ì£¼ëŠ”ê°€?
                        2. **ì •ë³´ ë¶€ì¡± ì—¬ë¶€**: ì‚¬ìš©ìì˜ ì˜ë„ë¥¼ ë§Œì¡±ì‹œí‚¤ê¸°ì— **ì •ë³´ê°€ ë¶€ì¡±**í•˜ì—¬, ì¶”ê°€ ì§ˆë¬¸(ì¥ë¥´, ë¶„ìœ„ê¸° ë“±)ì„ í•˜ëŠ” ê²ƒì´ ë” ë‚˜ì€ê°€?
                        3. **ë‹¤ì–‘ì„± ì£¼ì˜**: ê²°ê³¼ê°€ ë„ˆë¬´ ì¤‘êµ¬ë‚œë°©ì´ì–´ì„œ ì‚¬ìš©ìì—ê²Œ í˜¼ë€ì„ ì¤„ ê²ƒ ê°™ë‹¤ë©´ `sufficient: false`ë¡œ íŒë‹¨í•˜ë¼.
                        """
            strictness_instruction = "ì¶”ì²œí•˜ê¸°ì— ì¡°ê¸ˆì´ë¼ë„ ì• ë§¤í•˜ê±°ë‚˜, ì¶”ê°€ ì •ë³´ë¥¼ ë¬»ëŠ” ê²ƒì´ ì‚¬ìš©ìì—ê²Œ **ë” ë‚˜ì€ ì¶”ì²œ**ì„ ì¤„ ìˆ˜ ìˆë‹¤ë©´ ê³¼ê°í•˜ê²Œ `sufficient: false`ë¡œ íŒë‹¨í•˜ì„¸ìš”."

        system_prompt = f"""ë‹¹ì‹ ì€ ë„ì„œ ì¶”ì²œ ê²€ìƒ‰ ê²°ê³¼ì˜ í’ˆì§ˆì„ í‰ê°€í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

                        ê²€ìƒ‰ ê²°ê³¼ê°€ ì‚¬ìš©ì ìš”ì²­ì— **ì¶©ë¶„íˆ ì‘ë‹µ ê°€ëŠ¥í•œì§€** íŒë‹¨í•˜ì„¸ìš”.
                        
                        **í˜„ì¬ ëª¨í˜¸ì„± íƒ€ì…**: {ambiguity_type}

                        **í‰ê°€ ê¸°ì¤€**:
                        {criteria}

                        **ì£¼ì˜**: 
                        - {strictness_instruction}

                        JSON í˜•ì‹ìœ¼ë¡œ ë‹µí•˜ì„¸ìš”:
                        {{
                        "sufficient": true/false,
                        "reason": "íŒë‹¨ ê·¼ê±°",
                        "missing_info": ["ì¶”ê°€ë¡œ í•„ìš”í•œ ì •ë³´1"] (í•„ìˆ˜ì ì¸ ê²½ìš°ì—ë§Œ ì‘ì„±)
                        }}

                        missing_infoëŠ” `sufficient: false`ì¼ ë•Œ **ë°˜ë“œì‹œ** ì‘ì„±í•˜ì„¸ìš”."""

        user_message = f"""ì‚¬ìš©ì ìš”ì²­: {user_query}

                        ê²€ìƒ‰ëœ ì±…ë“¤:
                        {books_summary}

                        ì´ ê²°ê³¼ê°€ ì¶©ë¶„í•œê°€ìš”?"""

        model = self._get_model()

        messages = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=user_message)
        ]

        response = model.invoke(messages)

        # Parse JSON response
        import json
        try:
            content = response.content
            if "```json" in content:
                content = content.split("```json")[1].split("```")[0].strip()
            elif "```" in content:
                content = content.split("```")[1].split("```")[0].strip()

            result = json.loads(content)
            return result
        except (json.JSONDecodeError, IndexError):
            # Fallback: assume sufficient
            return {
                "sufficient": True,
                "reason": "íŒŒì‹± ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©",
                "missing_info": []
            }


class ClarificationQuestionGenerator:
    """Generates minimal, choice-based clarification questions."""

    def __init__(self, model_name: str = None):
        self.model_name = model_name or Config.CHAIN_MODEL_NAME
        self.model = None

    def _get_model(self):
        if self.model is None:
            self.model = init_chat_model(self.model_name)
        return self.model

    def generate(self, user_query: str, missing_info: List[str]) -> str:
        """
        Generate a single clarification question with choices.

        Args:
            user_query: Original user query
            missing_info: List of missing information pieces

        Returns:
            Clarification question with 2-3 choices
        """
        system_prompt = """ë‹¹ì‹ ì€ ëª…í™•í•œ ì§ˆë¬¸ì„ ë§Œë“œëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

                        **ì›ì¹™**:
                        1. ì§ˆë¬¸ì€ ë°˜ë“œì‹œ **1ê°œ**ë§Œ
                        2. **ì„ íƒì§€ í˜•íƒœ**ë¡œ ì œê³µ (2-3ê°œ ì˜µì…˜)
                        3. ì‚¬ìš©ìê°€ ë²ˆí˜¸ë‚˜ í‚¤ì›Œë“œë¡œ ì‰½ê²Œ ë‹µí•  ìˆ˜ ìˆê²Œ

                        ì˜ˆì‹œ:
                        "ì§€ê¸ˆ ìƒí™©ì— ë” ë§ëŠ” ìª½ì€ ì–´ëŠ ìª½ì¸ê°€ìš”?
                        1) ê°€ë³ê²Œ ì½íˆëŠ” ìœ„ë¡œ ìœ„ì£¼
                        2) ìƒê°í•  ê±°ë¦¬ë¥¼ ì£¼ëŠ” ë‚´ìš©"

                        ë˜ëŠ”:
                        "ì–´ë–¤ í˜•ì‹ì„ ì„ í˜¸í•˜ì‹œë‚˜ìš”?
                        1) ì†Œì„¤ (ì´ì•¼ê¸° ì¤‘ì‹¬)
                        2) ì—ì„¸ì´ (ì‚°ë¬¸ í˜•ì‹)
                        3) ì‹¤ìš©ì„œ (ì •ë³´ ì œê³µ)"

                        **ì˜¤ì§ ì§ˆë¬¸ê³¼ ì„ íƒì§€ë§Œ ì¶œë ¥í•˜ì„¸ìš”.**"""

        missing_str = ", ".join(missing_info)
        user_message = f"""ì‚¬ìš©ì ìš”ì²­: {user_query}

                        ë¶€ì¡±í•œ ì •ë³´: {missing_str}

                        ì´ ì¤‘ ê°€ì¥ ì¤‘ìš”í•œ ê²ƒ 1ê°œì— ëŒ€í•´ ì„ íƒì§€ ì§ˆë¬¸ì„ ë§Œë“œì„¸ìš”."""

        model = self._get_model()

        messages = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=user_message)
        ]

        response = model.invoke(messages)
        return response.content.strip()


class FinalRecommender:
    """Generates final book recommendation with context."""

    def __init__(self, model_name: str = None):
        # Use main chat model for final recommendation (needs better quality)
        self.model_name = model_name or Config.CHAT_MODEL_NAME
        self.model = None

    def _get_model(self):
        if self.model is None:
            self.model = init_chat_model(self.model_name)
        return self.model

    def recommend(
        self,
        user_query: str,
        retrieved_books: List[Dict[str, Any]],
        user_state_summary: str = "",
        clarification_history: List[Dict[str, str]] = None,
        include_links: bool = True
    ) -> str:
        """
        Generate final recommendation.

        Args:
            user_query: Original user query
            retrieved_books: List of retrieved book documents
            user_state_summary: Summary of user's inferred state
            clarification_history: List of Q&A during clarification
            include_links: Whether to include purchase links (default: True)

        Returns:
            Recommendation text with purchase links
        """
        # Format books context
        books_context = "\n\n".join([
            f"[{book.get('êµ¬ë¶„', 'N/A')}] {book.get('ìƒí’ˆëª…', 'N/A')}\n"
            f"ì†Œê°œ: {book.get('ì±…ì†Œê°œ', 'N/A')[:200]}..."
            for book in retrieved_books
        ])

        # Format clarification history
        clarification_text = ""
        if clarification_history:
            clarification_text = "\n\nì¶”ê°€ í™•ì¸ ë‚´ìš©:\n" + "\n".join([
                f"Q: {item['question']}\nA: {item['answer']}"
                for item in clarification_history
            ])

        system_prompt = f"""ë‹¹ì‹ ì€ ì „ë¬¸ ë„ì„œ ì¶”ì²œ íë ˆì´í„°ì…ë‹ˆë‹¤.

                            **ì¶”ì²œ ì›ì¹™**:
                            1. ì´ ì¶”ì²œì€ **ë¶ˆì™„ì „í•œ ì‚¬ìš©ì ì •ë³´**ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•¨ì„ ì¸ì •
                            2. ì‚¬ìš©ì ì‘ë‹µì— ë”°ë¼ ì¶”ì²œì„ **ê°±ì‹ í•  ìˆ˜ ìˆìŒ**ì„ ëª…ì‹œ
                            3. ê° ì±…ì˜ íŠ¹ì§•ê³¼ ì¶”ì²œ ì´ìœ ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…
                            4. ë² ìŠ¤íŠ¸ì…€ëŸ¬ ìˆœìœ„ê°€ ìˆë‹¤ë©´ ì°¸ê³ í•˜ë˜, ë§¹ëª©ì ìœ¼ë¡œ ë”°ë¥´ì§€ ì•ŠìŒ

                            **ì‚¬ìš©ì í˜„ì¬ ìƒíƒœ ì¶”ë¡ **:
                            {user_state_summary if user_state_summary else "ëª…ì‹œì  ì •ë³´ ì—†ìŒ"}

                            **ì¶”ì²œ ìŠ¤íƒ€ì¼**:
                            - 2-3ê¶Œ ì¶”ì²œ
                            - ê° ì±…ë§ˆë‹¤: [ì¹´í…Œê³ ë¦¬] **ì œëª©** - ì¶”ì²œ ì´ìœ  (1-2ë¬¸ì¥)
                            - **ì¤‘ìš”**: ì±… ì œëª©ì€ ë°˜ë“œì‹œ **ë³¼ë“œì²´**ë¡œ í‘œì‹œí•˜ì„¸ìš” (ì˜ˆ: **ì±… ì œëª©**)
                            - ë”°ëœ»í•˜ê³  ì¹œê·¼í•œ ì–´ì¡°
                            - ë§ˆì§€ë§‰ì— "ë‹¤ë¥¸ ìŠ¤íƒ€ì¼ì„ ì›í•˜ì‹œë©´ ë§ì”€í•´ì£¼ì„¸ìš”" ì¶”ê°€"""

        user_message = f"""ì‚¬ìš©ì ìš”ì²­: {user_query}
                        {clarification_text}

                        ì¶”ì²œ ê°€ëŠ¥í•œ ì±…ë“¤:
                        {books_context}

                        ì´ ì¤‘ì—ì„œ ì‚¬ìš©ìì—ê²Œ ê°€ì¥ ì í•©í•œ ì±…ì„ ì¶”ì²œí•´ì£¼ì„¸ìš”.
                        **ì±… ì œëª©ì€ ë°˜ë“œì‹œ ë³¼ë“œì²´ë¡œ í‘œì‹œí•˜ì„¸ìš”.**"""

        model = self._get_model()

        messages = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=user_message)
        ]

        response = model.invoke(messages)
        recommendation_text = response.content.strip()

        # Add purchase links if requested
        if include_links:
            recommendation_text = self._add_purchase_links(recommendation_text, retrieved_books)

        return recommendation_text

    def _add_purchase_links(self, recommendation_text: str, retrieved_books: List[Dict[str, Any]]) -> str:
        """
        Add purchase links to recommendation text based on books mentioned in the final recommendation.

        Args:
            recommendation_text: Original recommendation text
            retrieved_books: List of retrieved book documents

        Returns:
            Recommendation text with purchase links appended
        """
        import re

        # Extract book titles that are bolded in the recommendation text (using **title** format)
        # Pattern: **ì±… ì œëª©** format
        mentioned_titles = re.findall(r'\*\*([^*]+)\*\*', recommendation_text)

        if not mentioned_titles:
            return recommendation_text

        # Create a mapping of book titles to book data for quick lookup
        book_map = {book.get('ìƒí’ˆëª…', ''): book for book in retrieved_books if book.get('ìƒí’ˆëª…')}

        # Generate links section
        links_section = "\n\n---\n\n### ğŸ›’ êµ¬ë§¤ ë§í¬\n\n"

        added_books = set()  # Track books we've already added to avoid duplicates

        for title in mentioned_titles:
            # Skip if this title was already added
            if title in added_books:
                continue

            # Find matching book in retrieved books (exact match or fuzzy match)
            matched_book = None

            # First try exact match
            if title in book_map:
                matched_book = book_map[title]
            else:
                # Try fuzzy match: check if the mentioned title is a substring of any retrieved book
                for book_title, book_data in book_map.items():
                    if title in book_title or book_title in title:
                        matched_book = book_data
                        break

            if matched_book:
                author = matched_book.get('ì €ì/ì•„í‹°ìŠ¤íŠ¸', None)
            else:
                # If no match found in retrieved books, still generate links based on the mentioned title
                author = None

            # Generate links
            links = generate_search_links(title, author)

            links_section += f"**{title}**\n"
            links_section += f"- [ğŸ“š YES24 ê²€ìƒ‰]({links['yes24']})\n"
            links_section += f"- [ğŸ“– ì•Œë¼ë”˜ ê²€ìƒ‰]({links['aladin']})\n\n"

            added_books.add(title)

        return recommendation_text + links_section
